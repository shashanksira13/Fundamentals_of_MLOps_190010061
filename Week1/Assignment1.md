## Part 1
### Q1
AIOps stands for Artificial Intelligence for IT operations. It is a paradigm shift that allows machines to solve 
IT issues without the need of human assistance or interaction. AIOPs uses machine learning and analytics to 
analyze big data obtained via different tools, which allows for issues to be spotted automatically and dealt 
with in real time.

AIOps can be defined as platforms that utilize big data, machine learning, and other advanced analytics technologies
to directly and indirectly enhance IT operations (such as monitoring, automation and service desk) functions with 
proactive, personal, and dynamic insight.  Put another way, AIOps refers to improving the way IT organizations 
manage data and information in their environments using artificial intelligence.

This is different from MLOps because MLOps is a multidisciplinary approach to managing machine learning algorithms
as ongoing products, each with its own continuous lifecycle. It's a discipline that aims to build, scale, and 
deploy algorithms to production consistently.It takes the DevOps methodology of continuous integration and 
continuous deployment and applies it to machine learning. As in traditional development, there is code that 
needs to be written and deployed, as well as bug testing to be done, and changes in user requirements to be 
accommodated. Specific to the topic of machine learning, models are being trained with data, and new data is 
introduced to retrain the models again and again.

Sources:
https://www.unraveldata.com/resources/dataops-aiops-and-mlops/
https://opensource.com/article/21/2/aiops-vs-mlops

### Q2
In the context of ML systems, interpretabilityis the ability to explain or to present in understandable 
terms to a human. Machine Learning models generally work as ‘Black Boxes’. This means that though we can get 
accurate predictions from them, we cannot clearly explain or identify the logic behind these predictions. 
But how do we go about extracting important insights from the models? What things are to be kept in mind 
and what features or tools will we need to achieve that? These are the important questions that come to 
mind when the issue of Model interpretability is raised.

The question that some people often ask is why aren’t we just content with the results of the model and 
why are we so hell-bent on knowing why a particular decision was made? A lot of this has to do with the 
impact that a model might have in the real world. For models which are merely meant to recommend movies will 
have a far less impact than the ones created to predict the outcome of a drug. In a way, we capture the 
world by collecting raw data and use that data to make further predictions. Essentially, Interpretability 
is just another layer on the model that helps humans to understand the process.
Some of the benefits that interpretability brings along are:
-Reliability
-Debugging
-Informing feature engineering
-Directing future data collection
-Informing human decision-making
-Building Trust

In case of simple models like decision tree, linear regression etc., it is easier to explain model. In the 
case of linear regression, the chosen model represent a curve/hyperplane with dimensions being the input and 
target variables. The hyperplane in essence means the solution of a problem which we are trying to solve.

Source:
https://towardsdatascience.com/interpretable-machine-learning-1dec0f2f3e6b


## Part 2
[Part2](https://user-images.githubusercontent.com/61078142/124983374-80a3ea00-e055-11eb-9667-1a0baaef6ecf.PNG)
